{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d434d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "test = pd.read_csv(r\"H:\\Personal Repos and Projects\\Binary Classification with dataset\\Binary-Classification-with-a-Bank-Dataset\\test.csv\")\n",
    "train = pd.read_csv(r\"H:\\Personal Repos and Projects\\Binary Classification with dataset\\Binary-Classification-with-a-Bank-Dataset\\train.csv\")\n",
    "test = test.drop(columns=['id'])\n",
    "train = train.drop(columns=['id']) \n",
    "\n",
    "X = train[train.columns[~train.columns.isin(['y'])]]\n",
    "X = pd.get_dummies(X)\n",
    "y = train['y'].to_list()\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        'max_depth': trial.suggest_int(\"max_depth\", 3, 30),  # Reduced from 50\n",
    "        'min_samples_split': trial.suggest_int(\"min_samples_split\", 2, 50),\n",
    "        'min_samples_leaf': trial.suggest_int(\"min_samples_leaf\", 1, 50),\n",
    "        'max_features': trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", 0.3, 0.5, 0.7, 0.9]),\n",
    "        'max_leaf_nodes': trial.suggest_int(\"max_leaf_nodes\", 10, 1000, log=True),\n",
    "        'min_impurity_decrease': trial.suggest_float(\"min_impurity_decrease\", 0.0, 0.1),  # Reduced\n",
    "        'bootstrap': trial.suggest_categorical(\"bootstrap\", [True, False]),\n",
    "        'criterion': trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"]),\n",
    "        'class_weight': trial.suggest_categorical(\"class_weight\", [None, \"balanced\", \"balanced_subsample\"]),\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    # Optional parameters\n",
    "    if params['bootstrap']:\n",
    "        params['oob_score'] = trial.suggest_categorical(\"oob_score\", [True, False])\n",
    "        params['max_samples'] = trial.suggest_float(\"max_samples\", 0.5, 1.0)\n",
    "    \n",
    "    model = RandomForestClassifier(**params)\n",
    "    \n",
    "    # 5-fold CV instead of single split\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
    "    \n",
    "    return scores.mean()  # Return mean CV score\n",
    "\n",
    "\n",
    "class EarlyStoppingCallback:\n",
    "    def __init__(self, patience=50, min_delta=0.0001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.trials_without_improvement = 0\n",
    "        self.best_value = None\n",
    "        \n",
    "    def __call__(self, study, trial):\n",
    "        if self.best_value is None:\n",
    "            self.best_value = study.best_value\n",
    "            return\n",
    "        \n",
    "        if study.best_value > self.best_value + self.min_delta:\n",
    "            self.best_value = study.best_value\n",
    "            self.trials_without_improvement = 0\n",
    "        else:\n",
    "            self.trials_without_improvement += 1\n",
    "        \n",
    "        if self.trials_without_improvement >= self.patience:\n",
    "            print(f\"Early stopping at trial {trial.number}. Best value: {study.best_value:.6f}\")\n",
    "            study.stop()\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    sampler=optuna.samplers.TPESampler(n_startup_trials=300, seed=42),\n",
    "    study_name=\"rf_kaggle\",\n",
    "    storage=\"sqlite:///rf_kaggle.db\",\n",
    "    load_if_exists=True\n",
    ")\n",
    "\n",
    "# Optimize\n",
    "early_stopping = EarlyStoppingCallback(patience=100, min_delta=0.0001)\n",
    "study.optimize(objective, n_trials=5000, callbacks=[early_stopping])\n",
    "\n",
    "print(\"\\nBest parameters:\", study.best_params)\n",
    "print(f\"Best CV score: {study.best_value:.6f}\")\n",
    "\n",
    "# Train final model on full data with best params\n",
    "best_model = RandomForestClassifier(**study.best_params)\n",
    "best_model.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f822b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-26 16:00:45,268] A new study created in memory with name: no-name-da3d4380-02b4-4629-9ece-4a531977f2e6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting EXHAUSTIVE LightGBM hyperparameter optimization\n",
      "   Dataset shape: (750000, 16)\n",
      "   Categorical features: 9\n",
      "   Numerical features: 7\n",
      "   Target distribution: 12.07% positive class\n",
      "\n",
      "‚è≥ This will take several hours for exhaustive search...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9933c9b8732e4f519f4c9aacf4f1f537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "test = pd.read_csv(r\"H:\\Personal Repos and Projects\\Binary Classification with dataset\\Binary-Classification-with-a-Bank-Dataset\\test.csv\")\n",
    "train = pd.read_csv(r\"H:\\Personal Repos and Projects\\Binary Classification with dataset\\Binary-Classification-with-a-Bank-Dataset\\train.csv\")\n",
    "test = test.drop(columns=['id'])\n",
    "train = train.drop(columns=['id']) \n",
    "\n",
    "X = train[train.columns[~train.columns.isin(['y'])]]\n",
    "\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Convert categoricals to 'category' dtype for LightGBM\n",
    "for col in categorical_cols:\n",
    "    X[col] = X[col].astype('category')\n",
    "\n",
    "y = train['y'].values\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    EXHAUSTIVE LightGBM hyperparameter search\n",
    "    All parameters that affect model performance\n",
    "    \"\"\"\n",
    "    \n",
    "    params = {\n",
    "        # Core Parameters\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt', 'dart', 'goss']),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 3000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3, log=True),\n",
    "        \n",
    "        # Tree Structure Parameters\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 5, 200),\n",
    "        'min_sum_hessian_in_leaf': trial.suggest_float('min_sum_hessian_in_leaf', 0.001, 10.0, log=True),\n",
    "        \n",
    "        # Sampling Parameters\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 0, 10),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "        'feature_fraction_bynode': trial.suggest_float('feature_fraction_bynode', 0.4, 1.0),\n",
    "        \n",
    "        # Regularization Parameters\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 0.000001, 10.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 0.000001, 10.0, log=True),\n",
    "        'min_gain_to_split': trial.suggest_float('min_gain_to_split', 0.0, 1.0),\n",
    "        'max_delta_step': trial.suggest_float('max_delta_step', 0.0, 10.0),\n",
    "        \n",
    "        # Binary Classification Specific\n",
    "        'is_unbalance': trial.suggest_categorical('is_unbalance', [True, False]),\n",
    "        'scale_pos_weight': trial.suggest_float('scale_pos_weight', 0.5, 10.0) if not trial.params.get('is_unbalance', False) else 1.0,\n",
    "        \n",
    "        # Categorical Feature Parameters\n",
    "        'cat_smooth': trial.suggest_float('cat_smooth', 1.0, 100.0),\n",
    "        'cat_l2': trial.suggest_float('cat_l2', 0.0, 100.0),\n",
    "        'min_data_per_group': trial.suggest_int('min_data_per_group', 1, 200),\n",
    "        'max_cat_threshold': trial.suggest_int('max_cat_threshold', 1, 255),\n",
    "        \n",
    "        # Advanced Tree Parameters\n",
    "        'path_smooth': trial.suggest_float('path_smooth', 0.0, 10.0),\n",
    "        'min_child_weight': trial.suggest_float('min_child_weight', 0.001, 10.0, log=True),\n",
    "        'max_bin': trial.suggest_int('max_bin', 100, 512),\n",
    "        \n",
    "        # Speed/Memory Parameters (also affect accuracy)\n",
    "        'histogram_pool_size': trial.suggest_float('histogram_pool_size', -1.0, 16384.0),\n",
    "        \n",
    "        # Other Parameters\n",
    "        'extra_trees': trial.suggest_categorical('extra_trees', [True, False]),\n",
    "        'early_stopping_rounds': 50,\n",
    "        'verbose': -1,\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    # Special parameters for DART\n",
    "    if params['boosting_type'] == 'dart':\n",
    "        params['drop_rate'] = trial.suggest_float('drop_rate', 0.0, 0.3)\n",
    "        params['max_drop'] = trial.suggest_int('max_drop', 1, 100)\n",
    "        params['skip_drop'] = trial.suggest_float('skip_drop', 0.0, 1.0)\n",
    "        params['xgboost_dart_mode'] = trial.suggest_categorical('xgboost_dart_mode', [True, False])\n",
    "        params.pop('early_stopping_rounds')  # DART doesn't support early stopping\n",
    "    \n",
    "    # Special parameters for GOSS\n",
    "    if params['boosting_type'] == 'goss':\n",
    "        params['top_rate'] = trial.suggest_float('top_rate', 0.1, 0.5)\n",
    "        params['other_rate'] = trial.suggest_float('other_rate', 0.01, 0.3)\n",
    "        # GOSS doesn't support bagging\n",
    "        params.pop('bagging_fraction', None)\n",
    "        params.pop('bagging_freq', None)\n",
    "    \n",
    "    # Constraint handling\n",
    "    if params['num_leaves'] > 2**params['max_depth']:\n",
    "        params['num_leaves'] = 2**params['max_depth'] - 1\n",
    "    \n",
    "    # 5-Fold Cross Validation with early stopping per fold\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = []\n",
    "    models = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n",
    "        X_train_cv, X_val_cv = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train_cv, y_val_cv = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Create LightGBM datasets\n",
    "        train_data = lgb.Dataset(\n",
    "            X_train_cv, \n",
    "            label=y_train_cv, \n",
    "            categorical_feature=categorical_cols\n",
    "        )\n",
    "        \n",
    "        val_data = lgb.Dataset(\n",
    "            X_val_cv, \n",
    "            label=y_val_cv, \n",
    "            categorical_feature=categorical_cols,\n",
    "            reference=train_data\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        callbacks = [lgb.log_evaluation(0), lgb.early_stopping(50)] if params['boosting_type'] != 'dart' else []\n",
    "        \n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            train_data,\n",
    "            valid_sets=[val_data],\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        \n",
    "        # Predict and score\n",
    "        preds = model.predict(X_val_cv, num_iteration=model.best_iteration if params['boosting_type'] != 'dart' else None)\n",
    "        score = roc_auc_score(y_val_cv, preds)\n",
    "        cv_scores.append(score)\n",
    "        models.append(model)\n",
    "        \n",
    "        # Report intermediate value for Optuna pruning\n",
    "        intermediate_value = np.mean(cv_scores)\n",
    "        trial.report(intermediate_value, fold)\n",
    "        \n",
    "        # Prune bad trials early\n",
    "        if trial.should_prune():\n",
    "            # Clean up memory\n",
    "            del train_data, val_data, model\n",
    "            return intermediate_value\n",
    "    \n",
    "    # Store best model info for later use\n",
    "    trial.set_user_attr('cv_scores', cv_scores)\n",
    "    trial.set_user_attr('cv_std', np.std(cv_scores))\n",
    "    trial.set_user_attr('best_iteration', np.mean([m.best_iteration for m in models if hasattr(m, 'best_iteration')]))\n",
    "    \n",
    "    # Clean up memory\n",
    "    del models\n",
    "    \n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "\n",
    "# Advanced early stopping callback\n",
    "class ExhaustiveEarlyStopping:\n",
    "    def __init__(self, patience=100, min_delta=0.00001, min_trials=500):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.min_trials = min_trials\n",
    "        self.best_value = None\n",
    "        self.trials_without_improvement = 0\n",
    "        self.best_std = float('inf')\n",
    "        \n",
    "    def __call__(self, study, trial):\n",
    "        # Always run minimum trials\n",
    "        if trial.number < self.min_trials:\n",
    "            if trial.number % 100 == 0:\n",
    "                print(f\"\\n Trial {trial.number}\")\n",
    "                print(f\"   Best AUC: {study.best_value:.6f}\")\n",
    "                if 'cv_std' in study.best_trial.user_attrs:\n",
    "                    print(f\"   Best Std: {study.best_trial.user_attrs['cv_std']:.6f}\")\n",
    "            return\n",
    "        \n",
    "        current_value = trial.value\n",
    "        current_std = trial.user_attrs.get('cv_std', float('inf'))\n",
    "        \n",
    "        # Check if we have improvement (considering both mean and std)\n",
    "        has_improvement = False\n",
    "        if self.best_value is None:\n",
    "            has_improvement = True\n",
    "        elif current_value > self.best_value + self.min_delta:\n",
    "            has_improvement = True\n",
    "        elif abs(current_value - self.best_value) < self.min_delta and current_std < self.best_std:\n",
    "            has_improvement = True  # Same mean but lower variance is better\n",
    "        \n",
    "        if has_improvement:\n",
    "            self.best_value = current_value\n",
    "            self.best_std = current_std\n",
    "            self.trials_without_improvement = 0\n",
    "            print(f\"\\n Trial {trial.number}: New best = {current_value:.6f} (std: {current_std:.6f})\")\n",
    "        else:\n",
    "            self.trials_without_improvement += 1\n",
    "        \n",
    "        # Detailed progress every 100 trials\n",
    "        if trial.number % 100 == 0:\n",
    "            print(f\"\\n Progress Report - Trial {trial.number}\")\n",
    "            print(f\"   Current: {current_value:.6f} (std: {current_std:.6f})\")\n",
    "            print(f\"   Best: {self.best_value:.6f} (std: {self.best_std:.6f})\")\n",
    "            print(f\"   No improvement for: {self.trials_without_improvement} trials\")\n",
    "            print(f\"   Boosting type: {trial.params.get('boosting_type', 'gbdt')}\")\n",
    "        \n",
    "        # Stop if no improvement\n",
    "        if self.trials_without_improvement >= self.patience:\n",
    "            print(f\"\\n Early stopping triggered at trial {trial.number}\")\n",
    "            print(f\"   No improvement for {self.patience} trials\")\n",
    "            print(f\"   Final best AUC: {study.best_value:.6f}\")\n",
    "            study.stop()\n",
    "\n",
    "\n",
    "# Create and run the study\n",
    "print(\"Starting EXHAUSTIVE LightGBM hyperparameter optimization\")\n",
    "print(f\"   Dataset shape: {X.shape}\")\n",
    "print(f\"   Categorical features: {len(categorical_cols)}\")\n",
    "print(f\"   Numerical features: {len(numerical_cols)}\")\n",
    "print(f\"   Target distribution: {np.mean(y):.2%} positive class\\n\")\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    sampler=TPESampler(\n",
    "        n_startup_trials=500,  # Large random exploration phase\n",
    "        n_ei_candidates=100,   # More candidates for better optimization\n",
    "        seed=42\n",
    "    ),\n",
    "    pruner=MedianPruner(\n",
    "        n_startup_trials=100,\n",
    "        n_warmup_steps=2,  # Complete at least 2 CV folds before pruning\n",
    "        interval_steps=1\n",
    "    )\n",
    ")\n",
    "\n",
    "# Set up callbacks\n",
    "early_stopping = ExhaustiveEarlyStopping(\n",
    "    patience=150,  # More patience for exhaustive search\n",
    "    min_delta=0.00001,\n",
    "    min_trials=1000  # Minimum 1000 trials for exhaustive approach\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "print(\"This will take several hours for exhaustive search...\\n\")\n",
    "\n",
    "try:\n",
    "    study.optimize(\n",
    "        objective,\n",
    "        n_trials=10000,  # Maximum trials for exhaustive search\n",
    "        callbacks=[early_stopping],\n",
    "        gc_after_trial=True,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n Optimization interrupted by user\")\n",
    "\n",
    "# Results and analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OPTIMIZATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTrials completed: {len(study.trials)}\")\n",
    "print(f\"Best ROC-AUC Score: {study.best_value:.6f}\")\n",
    "if 'cv_std' in study.best_trial.user_attrs:\n",
    "    print(f\"Cross-validation Std: {study.best_trial.user_attrs['cv_std']:.6f}\")\n",
    "if 'best_iteration' in study.best_trial.user_attrs:\n",
    "    print(f\"Average best iteration: {study.best_trial.user_attrs['best_iteration']:.0f}\")\n",
    "\n",
    "print(\"\\nBest Parameters:\")\n",
    "for key, value in sorted(study.best_params.items()):\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Feature importance from best model\n",
    "print(\"\\n Training final model on full training data...\")\n",
    "best_params = study.best_params.copy()\n",
    "best_params.update({\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'verbose': -1,\n",
    "    'random_state': 42\n",
    "})\n",
    "\n",
    "final_train_data = lgb.Dataset(X, label=y, categorical_feature=categorical_cols)\n",
    "final_model = lgb.train(\n",
    "    best_params,\n",
    "    final_train_data,\n",
    "    num_boost_round=int(study.best_trial.user_attrs.get('best_iteration', 1000))\n",
    ")\n",
    "\n",
    "# Save everything\n",
    "print(\"\\n Saving results...\")\n",
    "final_model.save_model('best_lgbm_model.txt')\n",
    "study.trials_dataframe().to_csv('optuna_trials.csv', index=False)\n",
    "pd.DataFrame([study.best_params]).to_json('best_params.json')\n",
    "\n",
    "print(\"\\n Exhaustive optimization complete!\")\n",
    "print(f\"   Model saved to: best_lgbm_model.txt\")\n",
    "print(f\"   Trials saved to: optuna_trials.csv\")\n",
    "print(f\"   Best params saved to: best_params.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
